{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project 3#: Build a Traffic Sign Recognition Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports, notebook configuration and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = -1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Imports\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import urllib.request\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import skimage as ski\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import skimage.io\n",
    "import skimage.util\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn as skl\n",
    "import sklearn.utils\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# Adjust warnings and log verbosity\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "def update_progress(progress, opt_text=\"\"):\n",
    "    \"\"\"\n",
    "    Print a rewritable single line ASCI progress bar.\n",
    "    :param progress: Progress from 0.0 to 1.0.\n",
    "    \"\"\"\n",
    "    bar_length = 20\n",
    "    block = int(math.ceil(bar_length * progress))\n",
    "    print(\"Progress: [{}] {:.1f}%   {:s}\".format( \"#\" * block + \"-\" * (bar_length - block),\n",
    "                                                 progress * 100, opt_text), end='\\r')\n",
    "    \n",
    "def color_depth(dataset):\n",
    "    \"\"\"\n",
    "    Determine the image color depth of a dataset.\n",
    "    \"\"\"\n",
    "    return dataset.shape[-1] if (isinstance(dataset, np.ndarray) and dataset.ndim == 4) else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Step 0: Load The Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def region_of_interest(img, original_size, original_roi):\n",
    "    \"\"\"\n",
    "    Given the original image size and region of interest (ROI), calculate the ROI according to the current shape.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        original_roi[0] / original_size[0] * img.shape[0],\n",
    "        original_roi[1] / original_size[1] * img.shape[1],\n",
    "        original_roi[2] / original_size[0] * img.shape[0],\n",
    "        original_roi[3] / original_size[1] * img.shape[1],\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_from_pickle(file):\n",
    "    \"\"\"\n",
    "    Load the pickled traffic sign image dataset from Udacity\n",
    "    :param file: File handler to the pickle file.\n",
    "    :return: A tuple of features, labels and rescaled ROIs.\n",
    "    \"\"\"\n",
    "    data = pickle.load(file)\n",
    "    X, y = data['features'], data['labels']\n",
    "\n",
    "    assert(len(X) == len(y))\n",
    "\n",
    "    if 'coords' in data:\n",
    "        roi = np.zeros_like(data['coords'], dtype=np.float)\n",
    "        for i in range(len(roi)):\n",
    "            roi[i] = region_of_interest(data['features'][i], data['sizes'][i], data['coords'][i])\n",
    "    else:\n",
    "        roi = []\n",
    "\n",
    "    return X, y, roi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook's temporary data directory\n",
    "DATA_DIR = 'data'\n",
    "# URL pointing to Udacity's pickled traffic signs dataset\n",
    "ARCHIVE_URL = 'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/traffic-signs-data.zip'\n",
    "\n",
    "# Repository locations of the downloaded and extracted data\n",
    "training_file = DATA_DIR + '/train.p'\n",
    "validation_file = DATA_DIR + '/valid.p'\n",
    "testing_file = DATA_DIR + '/test.p'\n",
    "\n",
    "# If the pickle files do not exist yet, download and extract the archive\n",
    "if any(not os.path.exists(file) for file in (training_file, validation_file, testing_file)):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "    url = urllib.request.urlopen(ARCHIVE_URL)\n",
    "\n",
    "    with ZipFile(BytesIO(url.read())) as my_zip_file:\n",
    "        my_zip_file.extractall(DATA_DIR)\n",
    "\n",
    "# Load training, validation and testing data\n",
    "with open(training_file, mode='rb') as f:\n",
    "    X_train, y_train, roi_train = load_from_pickle(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    X_valid, y_valid, roi_valid = load_from_pickle(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    X_test, y_test, roi_test = load_from_pickle(f)\n",
    "\n",
    "print(\"Data successfully loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Dataset Summary & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Visualization of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the class index -> class name mapping file\n",
    "SIGNNAMES_FILE = 'signnames.csv'\n",
    "\n",
    "def summarize_dataset(train=([],[],[]), valid=([],[],[]), test=([],[],[]), plot_distribution=True):\n",
    "    \"\"\"\n",
    "    Given the loaded features, lables and ROIs for training, validation and testing data,\n",
    "    print some basic metrics and visualize the dataset.\n",
    "    :return: A list of class indices\n",
    "    \"\"\"\n",
    "    (X_train, y_train, roi_train) = train\n",
    "    (X_valid, y_valid, roi_valid) = valid\n",
    "    (X_test, y_test, roi_test) = test\n",
    "\n",
    "    # Assess unique classes and distribution of the training dataset using NumPy\n",
    "    classes, classes_count = np.unique(y_test, return_counts=True)\n",
    "\n",
    "    # Number of training examples\n",
    "    n_train = len(y_train)\n",
    "    # Number of validation examples\n",
    "    n_validation = len(y_valid)\n",
    "    # Number of testing examples.\n",
    "    n_test = len(y_test)\n",
    "    # What's the shape of an traffic sign image?\n",
    "    image_shape = X_test[0].shape if len(X_test) else X_train[0].shape\n",
    "    # How many unique classes/labels there are in the dataset.\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Print basic summary\n",
    "    print(\"Number of training examples =\", n_train)\n",
    "    print(\"Number of validation examples =\", n_validation)\n",
    "    print(\"Number of testing examples =\", n_test)\n",
    "    print(\"Image data shape =\", image_shape)\n",
    "    print(\"Number of classes =\", n_classes)\n",
    "\n",
    "    # Plot a histogram, showing the distribution of samples across classes using Pandas\n",
    "    classes_df = pd.read_csv(SIGNNAMES_FILE, index_col=0)\n",
    "    classes_df['Training'] = pd.Series(y_train).value_counts().sort_index()\n",
    "    classes_df['Validation'] = pd.Series(y_valid).value_counts().sort_index()\n",
    "    classes_df['Testing']  = pd.Series(y_test) .value_counts().sort_index()\n",
    "\n",
    "    if plot_distribution:\n",
    "        classes_df.plot(kind='bar', stacked=True, figsize=(14,5))\n",
    "        # print(classes_df)\n",
    "\n",
    "    # prepare a grid of subplots\n",
    "    ncols = 5\n",
    "    nrows = (n_classes + ncols - 1) // ncols\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14,nrows*2.2), squeeze=True)\n",
    "\n",
    "    # iterate over all subplot axes and image classes\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        # i runs from 0 to (nrows*ncols-1)\n",
    "        if i >= n_classes:\n",
    "            fig.delaxes(axi)\n",
    "            continue\n",
    "\n",
    "        cls_id = classes[i]\n",
    "        cls_title = classes_df.iloc[:,0][cls_id]\n",
    "        cls_count = np.nansum([classes_df.iloc[:,k][cls_id] for k in (1,2,3)], dtype=np.uint32)\n",
    "\n",
    "        # select an image for plotting\n",
    "        indices = np.where(y_test==cls_id)[0]\n",
    "        sel_index = np.random.choice(indices)\n",
    "        img = X_test[sel_index]\n",
    "\n",
    "        # plot the image along with the class index, title and number of samples per class\n",
    "        axi.imshow(img)\n",
    "        axi.set_title(\"{:02d}: {:20.20s} [{}]\".format(cls_id, cls_title, cls_count))\n",
    "\n",
    "        # draw the rectangular region of interest onto the image\n",
    "        if len(roi_test) > 0:\n",
    "            roi = roi_test[sel_index]\n",
    "            axi.add_patch(Rectangle(\n",
    "                (roi[0], roi[1]), roi[2]-roi[0], roi[3]-roi[1],\n",
    "                linewidth=1,\n",
    "                edgecolor='r',\n",
    "                facecolor='none'\n",
    "            ))\n",
    "\n",
    "    plt.tight_layout(pad=1.1)\n",
    "    plt.show()\n",
    "\n",
    "    return classes\n",
    "\n",
    "# print summary and plots\n",
    "classes = summarize_dataset(\n",
    "    (X_train, y_train, roi_train),\n",
    "    (X_valid, y_valid, roi_valid),\n",
    "    (X_test, y_test, roi_test)\n",
    ")\n",
    "\n",
    "# number of available classes of traffic signs\n",
    "n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "### Pre-process the Dataset (normalization, grayscale, rescaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "def pad(img, shape):\n",
    "    \"\"\"\n",
    "    Pad the given image to fill a specific shape.\n",
    "    The last pixel of the image is used for padding.\n",
    "    \"\"\"\n",
    "    if img.shape >= shape:\n",
    "        return img\n",
    "\n",
    "    out = np.full(shape, np.mean(img.flat[-5:]))\n",
    "    out[tuple(slice(0, d) for d in np.shape(img))] = img\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess_images(dataset, dataset_roi=None, size=(32,32), debug=-1):\n",
    "    \"\"\"\n",
    "    Preprocess a given image dataset.\n",
    "\n",
    "    The following stages are executed:\n",
    "    * Conversion to grayscale, if applicable.\n",
    "    * If the region of interest ``dataset_roi`` is given, each image is cropped to that region,\n",
    "      then padded and resized to match ``size``.\n",
    "    * Normalization: The available color range for each image is rescaled to the range [-1.0, 1.0]\n",
    "    * The dataset is reshaped to be suitable for tensorflow input.\n",
    "\n",
    "    :return: The processed image dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Processing image dataset ...\")\n",
    "    processed_data = np.zeros((len(dataset), size[0], size[1], 1), np.float)\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "\n",
    "        img = dataset[i]\n",
    "\n",
    "        if i == debug:\n",
    "            plt.figure()\n",
    "            plt.imshow(img)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        if img.shape[-1] == 3:\n",
    "            img = ski.color.rgb2gray(img)\n",
    "\n",
    "        img = img.astype(np.float)\n",
    "\n",
    "        # Crop and resize to region of interest\n",
    "        if not dataset_roi is None and i < len(dataset_roi):\n",
    "\n",
    "            roi = dataset_roi[i]\n",
    "\n",
    "            if i == debug:\n",
    "                plt.figure()\n",
    "                plt.imshow(img, cmap=\"gray\")\n",
    "                plt.gca().add_patch(Rectangle((roi[0],roi[1]),roi[2]-roi[0],roi[3]-roi[1],linewidth=1,edgecolor='r',facecolor='none'))\n",
    "\n",
    "            # Calculate the area to crop, leaving some padding outside the given ROI on each side\n",
    "            # of the reactangle\n",
    "            \n",
    "            w = roi[2]-roi[0]\n",
    "            h = roi[3]-roi[1]\n",
    "            \n",
    "            padding = 0.09\n",
    "            \n",
    "            # croop coordinates\n",
    "            x1 = math.floor(roi[0] - w*padding)\n",
    "            x2 = math.ceil( roi[2] + w*padding) + 1\n",
    "            x1 = max(x1, 0)\n",
    "            x2 = min(x2, img.shape[1])\n",
    "            p = x2-x1\n",
    "            \n",
    "            y1 = math.floor(roi[1] - h*padding)\n",
    "            y2 = math.ceil( roi[3] + h*padding) + 1\n",
    "            y1 = max(y1, 0)\n",
    "            y2 = min(y2, img.shape[0])\n",
    "            q = y2-y1\n",
    "            \n",
    "#             print(roi, (x1,y1,x2,y2), (img.shape[1],img.shape[0]))\n",
    "            \n",
    "            # If the calculated crop area does differ from the target image size go ahead.\n",
    "            if size != (x2-x1, y2-y1):\n",
    "\n",
    "                # crop the image\n",
    "                img_cropped = img[y1:y2,x1:x2]\n",
    "                \n",
    "                if img_cropped.size > 0:\n",
    "\n",
    "                    if i == debug:\n",
    "                        plt.figure()\n",
    "                        plt.imshow(img_cropped, cmap=\"gray\")\n",
    "\n",
    "                    # pad the image if necessary\n",
    "#                     img = pad(img_cropped, (max(img_cropped.shape),max(img_cropped.shape)))\n",
    "\n",
    "                    # resize the cropped image to match the target size again\n",
    "                    img = ski.transform.resize(img_cropped, (size[0],size[1]), anti_aliasing=True)\n",
    "\n",
    "                    if i == debug:\n",
    "                        plt.figure()\n",
    "                        plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "        # Normalize image data to a range [-1.0, +1.0]\n",
    "        col_range = np.max(img) - np.min(img)\n",
    "        img = (img - np.min(img)) / col_range * 2 - 1\n",
    "\n",
    "        if i == debug:\n",
    "            plt.figure()\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "            \n",
    "        # Reshape for tensor input\n",
    "        img = img.reshape((32, 32, 1))\n",
    "\n",
    "        processed_data[i] = img\n",
    "\n",
    "        # print progress bar\n",
    "        if i % 1000 == 0 or i == len(dataset)-1:\n",
    "            update_progress((i+1) / len(dataset))\n",
    "\n",
    "    print()\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "X_train = preprocess_images(X_train, roi_train)\n",
    "X_valid = preprocess_images(X_valid, roi_valid)\n",
    "X_test = preprocess_images(X_test, roi_test)\n",
    "\n",
    "# Determine input color depth\n",
    "input_depth = color_depth(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Augment and Balance the Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_image(img):\n",
    "    \"\"\"\n",
    "    Transform a single image by applying a random amount of rotation between [-15°,15°] and a random\n",
    "    amount of Gaussian noise.\n",
    "    \"\"\"\n",
    "\n",
    "    result = np.zeros_like(img)\n",
    "\n",
    "    # apply a random amount of rotation between -15 and 15 degrees\n",
    "    random_degree = random.uniform(-15, 15)\n",
    "    result = ski.transform.rotate(img, random_degree)\n",
    "\n",
    "    # apply a random amount of gaussian noise (no clipping)\n",
    "    random_var = random.uniform(0.0, 0.01)\n",
    "    result = ski.util.random_noise(result, var=random_var, clip=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def augment_data(X, y, roi, factor=2, debug=-1):\n",
    "    \"\"\"\n",
    "    Augment the preprocessed image dataset.\n",
    "\n",
    "    Source images are picked randomly within each image class to first balance the dataset.\n",
    "    In a second pass, images are picked evenly across classes to increase the size of the balanced\n",
    "    dataset by a given `factor`.\n",
    "    Each copied image is applied a random amount of Gaussian noise and rotation using the function\n",
    "    `transform_image()`.\n",
    "    :return: The augmented dataset as tuple (features, labels and roi).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Augmenting and balancing dataset ...\")\n",
    "\n",
    "    # Assess unique classes and distribution of the training dataset\n",
    "    classes, classes_count = np.unique(y, return_counts=True)\n",
    "    # print(classes_count)\n",
    "\n",
    "    # calculate the augmented dataset size\n",
    "    n_per_class_max = np.nanmax(classes_count)\n",
    "    final_dataset_size = n_per_class_max * len(classes) * factor\n",
    "    augmented_size = final_dataset_size - len(X)\n",
    "\n",
    "    # prepare result arrays\n",
    "    X_augmented = np.zeros((augmented_size,) + X.shape[1:])\n",
    "    y_augmented = np.zeros((augmented_size,) + y.shape[1:])\n",
    "    roi_augmented = np.zeros((augmented_size,) + roi.shape[1:])\n",
    "\n",
    "    # counter for images added to `X_augmented`\n",
    "    j = 0\n",
    "    # iterate over all classes and augment within tha class (choose images according to dataset imbalance)\n",
    "    for (cls, count) in zip(classes, classes_count):\n",
    "        for i in range(count, n_per_class_max*factor):\n",
    "            # pick a random image from the source dataset, but with label == cls:\n",
    "            p = np.random.choice(np.where(y == cls)[0])\n",
    "\n",
    "            if j == debug:\n",
    "                plt.figure()\n",
    "                plt.imshow(X[p], cmap=\"gray\")\n",
    "\n",
    "            # transform the image\n",
    "            X_augmented[j] = transform_image(X[p])\n",
    "\n",
    "            if j == debug:\n",
    "                plt.figure()\n",
    "                plt.imshow(X_augmented[j] , cmap=\"gray\")\n",
    "\n",
    "            # keep the label and roi for this new addition to the dataset\n",
    "            y_augmented[j] = y[p]\n",
    "            roi_augmented[j] = roi[p]\n",
    "            j += 1\n",
    "\n",
    "            # print progress\n",
    "            if j % 1000 == 0 or j == augmented_size-1:\n",
    "                update_progress((j+1) / augmented_size)\n",
    "\n",
    "    print()\n",
    "    # Concetenate source and augmented dataset and return\n",
    "    return np.concatenate((X, X_augmented)), np.concatenate((y, y_augmented)), np.concatenate((roi, roi_augmented))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "# Print imbalance ratios for augmented training dataset\n",
    "classes, classes_count = np.unique(y_train, return_counts=True)\n",
    "print(\"Minimum/maximum training data class count: {} / {}\".format(min(classes_count), max(classes_count)))\n",
    "print()\n",
    "\n",
    "# Augment the dataset\n",
    "X_train, y_train, roi_train = augment_data(X_train, y_train, roi_train)\n",
    "print(\"Training dataset augmented. Number of examples = {}\".format(len(y_train)))\n",
    "print()\n",
    "\n",
    "# Print imbalance ratios for augmented training dataset\n",
    "classes, classes_count = np.unique(y_train, return_counts=True)\n",
    "print(\"Minimum/maximum training data class count: {} / {}\".format(min(classes_count), max(classes_count)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function around tf.nn.conv2d so simplify 2D convolutions\n",
    "def conv2d(x, W, b, strides=1, padding='VALID', name=None):\n",
    "    conv = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=padding, name=name)\n",
    "    return tf.nn.bias_add(conv, b)\n",
    "\n",
    "\n",
    "# Helper function around tf.nn.max_pool to simplify max pooling\n",
    "def pool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='VALID')\n",
    "\n",
    "\n",
    "def LeNet(x):\n",
    "    \"\"\"\n",
    "    Tensor flow model for a convoluted neural network, following the LeNet-5 architecture.\n",
    "    :param x: Tensor holding an array of input images with shape (32,32,1).\n",
    "    :return: Tensor holding the result logits for the given input.\n",
    "    \"\"\"\n",
    "\n",
    "    # Arguments used for weight initialization via `tf.truncated_normal`\n",
    "    MU = 0\n",
    "    SIGMA = 0.1\n",
    "\n",
    "    # Make the convolutional layer accessible outside for visualization\n",
    "    global conv1, conv2, fc2_w, fc3_w\n",
    "\n",
    "    # Layer 1: Convolutional. Input = 32x32xinput_depth. Output = 28x28x6.\n",
    "    conv1_w = tf.Variable(tf.truncated_normal([5, 5, input_depth, 6], MU, SIGMA))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1 = conv2d(x, conv1_w, conv1_b)\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.tanh(conv1)\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1_p = pool2d(conv1, 2)\n",
    "\n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_w = tf.Variable(tf.truncated_normal([5, 5, 6, 16], MU, SIGMA))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2 = conv2d(conv1_p, conv2_w, conv2_b)\n",
    "    # Activation.\n",
    "    conv2 = tf.nn.tanh(conv2)\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2_p = pool2d(conv2, 2)\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0 = tf.contrib.layers.flatten(conv2_p)\n",
    "    # 1st Dropout.\n",
    "    fc0 = tf.nn.dropout(fc0, rate=dropout_rate)\n",
    "\n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_w = tf.Variable(tf.truncated_normal([400, 120], MU, SIGMA))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1 = tf.add(tf.matmul(fc0, fc1_w), fc1_b)\n",
    "    # Activation.\n",
    "    fc1 = tf.nn.tanh(fc1)\n",
    "    # 2nd Dropout.\n",
    "    fc1 = tf.nn.dropout(fc1, rate=dropout_rate)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_w = tf.Variable(tf.truncated_normal([120, 84], MU, SIGMA))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2 = tf.add(tf.matmul(fc1, fc2_w), fc2_b)\n",
    "    # Activation.\n",
    "    fc2 = tf.nn.tanh(fc2)\n",
    "    # 3rd Dropout.\n",
    "    fc2 = tf.nn.dropout(fc2, rate=dropout_rate)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = n_classes.\n",
    "    fc3_w = tf.Variable(tf.truncated_normal([84, n_classes], MU, SIGMA))\n",
    "    fc3_b = tf.Variable(tf.zeros(n_classes))\n",
    "    logits = tf.add(tf.matmul(fc2, fc3_w), fc3_b)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model\n",
    "\n",
    "#### Hyperparameters and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "LEARNING_RATE = 0.0005\n",
    "# Number of epochs\n",
    "EPOCHS = 200\n",
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Probability of dropping input nodes in dropout layers (only used during training, otherwise = 0.0)\n",
    "DROPOUT_RATE = 0.3\n",
    "# Multiplier for L2 regularization (deactivated if 0.0)\n",
    "BETA = 0.0\n",
    "\n",
    "# Probability of dropping input elements to dropout nodes\n",
    "dropout_rate = tf.placeholder_with_default(0.0, ())\n",
    "# Placeholder for input features\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, input_depth))\n",
    "# Placeholder for input labels (scalar)\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "# One hot tensor representation of labels\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "\n",
    "\n",
    "# Logits as defined by our LeNet model\n",
    "logits = LeNet(x)\n",
    "# Cross Entropy (incl. Soft-Max)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "# Summation and mean\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# L2 regularization (if BETA > 0)\n",
    "if BETA > 0.0:\n",
    "    regularizer = (tf.nn.l2_loss(fc2_w) + tf.nn.l2_loss(fc3_w))\n",
    "    loss_operation = tf.reduce_mean(loss_operation + BETA * regularizer)\n",
    "\n",
    "# Optimizer function\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE)\n",
    "# Training op. for the TF runs\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "\n",
    "# Operation returning the label with the highest logits score (the prediction)\n",
    "prediction = tf.argmax(logits, 1)\n",
    "# True if the prediction matches the input label\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(one_hot_y, 1))\n",
    "# Accuracy averaged over a batch of inputs\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "def predict(X_data):\n",
    "    \"\"\"\n",
    "    Calculate the predictions for an image dataset, using tf.Session.run() in batches.\n",
    "    \"\"\"\n",
    "    sess = tf.get_default_session()\n",
    "    result = np.zeros(len(X_data), dtype=np.uint8)\n",
    "    \n",
    "    for offset in range(0, len(X_data), BATCH_SIZE):\n",
    "        batch_x = X_data[offset:offset+BATCH_SIZE]\n",
    "        result[offset:offset+BATCH_SIZE] = sess.run(prediction, feed_dict={x: batch_x, dropout_rate: 0.0})\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    \"\"\"\n",
    "    Calculate the mean accuracy and loss for classifying an image dataset, using tf.Session.run() in batches.\n",
    "    \"\"\"\n",
    "    sess = tf.get_default_session()\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for offset in range(0, len(X_data), BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        \n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, dropout_rate: 0.0})\n",
    "        loss = sess.run(loss_operation, feed_dict={x: batch_x, y: batch_y, dropout_rate: 0.0})\n",
    "        \n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        total_loss += (loss * len(batch_x))\n",
    "        \n",
    "    return total_accuracy / len(X_data), total_loss / len(X_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training and Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Store training and validation accuracy / loss per epoch\n",
    "    training_accs = []\n",
    "    validation_accs = []\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    print(\"Training model ...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = skl.utils.shuffle(X_train, y_train)\n",
    "        for offset in range(0, len(X_train), BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, dropout_rate: DROPOUT_RATE})\n",
    "\n",
    "        training_accuracy, training_loss = evaluate(X_train, y_train)\n",
    "        validation_accuracy, validation_loss = evaluate(X_valid, y_valid)\n",
    "\n",
    "        # store training/validation accuracies and losses for each epoch for later visualization\n",
    "        training_accs.append(training_accuracy)\n",
    "        validation_accs.append(validation_accuracy)\n",
    "        training_losses.append(training_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "        # print progress\n",
    "        update_progress((i+1) / EPOCHS, \"({}: {:.3f}/{:.3f})\".format((i+1), training_accuracy, validation_accuracy))\n",
    "\n",
    "    # print final accuracies\n",
    "    print()\n",
    "    print()\n",
    "    print(\"EPOCH {}:\".format(EPOCHS))\n",
    "    print(\"Training Accuracy = {:.3f}\".format(training_accuracy))\n",
    "    print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "    print()\n",
    "\n",
    "    # save the model\n",
    "    saver.save(sess, DATA_DIR+'/lenet')\n",
    "    print(\"Model saved.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training/validation accuracies and losses vs. epoch evolution to check for overfitting\n",
    "if validation_accs:\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12,7), sharex=True, squeeze=True)\n",
    "    \n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].plot(range(1, len(training_accs)+1), training_accs, '.-b', label='Training')\n",
    "    ax[0].plot(range(1, len(validation_accs)+1), validation_accs, '.-r', label='Validation')\n",
    "    ax[0].set_ylim([0.95, 1.0])\n",
    "    ax[0].legend(loc='lower right')\n",
    "    ax[0].grid()\n",
    "#     ax[0].set_xlabel('Epoch')\n",
    "    \n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].plot(range(1, len(training_losses)+1), training_losses, '.-b', label='Training')\n",
    "    ax[1].plot(range(1, len(validation_losses)+1), validation_losses, '.-r', label='Validation')\n",
    "    ax[1].set_ylim([0.0, 0.2])\n",
    "    ax[1].legend(loc='upper right')\n",
    "    ax[1].grid()\n",
    "    ax[1].set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Restore session\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(DATA_DIR))\n",
    "\n",
    "    # Calculate accuracy for the test data\n",
    "    test_accuracy, _ = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "\n",
    "    # Get the predictions for the test data\n",
    "    p_test = predict(X_test)\n",
    "    \n",
    "    # Construct a confusion matrix\n",
    "    cfm = tf.math.confusion_matrix(y_test, p_test).eval(session=sess)\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    \n",
    "    # Print parts of the confusion matrix\n",
    "    print()\n",
    "    print(\"Confusion matrix [0:15,0:15]\")\n",
    "    print(cfm[:15,:15])\n",
    "    print()\n",
    "    print(\"Confusion matrix [15:30,15:30]\")\n",
    "    print(cfm[15:30,15:30])\n",
    "    print()\n",
    "    print(\"Confusion matrix [30:43,30:43]\")\n",
    "    print(cfm[30:,30:])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for selected German traffic sign web images\n",
    "IMAGE_DIR = 'images'\n",
    "\n",
    "# Filenames of those images\n",
    "filenames = [\n",
    "#     'sign_20.jpg',\n",
    "#     'sign_30.jpg',\n",
    "    'sign_100.jpg',\n",
    "    'sign_stop.jpg',\n",
    "    'sign_noentry.jpg',\n",
    "    'sign_children.jpg',\n",
    "#     'sign_straight_turn_left.jpg',\n",
    "    'sign_roundabout.jpg',\n",
    "]\n",
    "\n",
    "# Labels\n",
    "y_new = np.array([\n",
    "#      0,\n",
    "#      1,\n",
    "     7,\n",
    "    14,\n",
    "    17,\n",
    "    28,\n",
    "#     37,\n",
    "    40\n",
    "])\n",
    "\n",
    "# Region of interest for the above images\n",
    "roi_new = np.array([\n",
    "#     (120, 125, 470, 477),\n",
    "#     ( 41,  28, 154, 137),\n",
    "    ( 23,   8, 332, 304),\n",
    "    (266,   7, 882, 613),\n",
    "    (116,  48, 507, 467),\n",
    "    ( 49,   9, 389, 302),\n",
    "#     ( 33,  22, 168, 149),\n",
    "    ( 68,  71, 236, 239)\n",
    "])\n",
    "\n",
    "# Load images and construct feature array\n",
    "X_new = []\n",
    "for fn in filenames:\n",
    "    img = ski.io.imread(IMAGE_DIR+'/'+fn, as_gray=False)\n",
    "    img = ski.util.img_as_ubyte(img)\n",
    "    X_new.append(img)\n",
    "\n",
    "# Summarize the new dataset\n",
    "summarize_dataset(test=(X_new, y_new, roi_new), plot_distribution=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process the New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new images\n",
    "X_new = preprocess_images(X_new, roi_new)\n",
    "\n",
    "# Determine input depth\n",
    "input_depth = color_depth(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classify the new images\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(DATA_DIR))\n",
    "\n",
    "    p_new = predict(X_new)\n",
    "    print(\"New Data Predictions:\", p_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the accuracy for these 5 new images.\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(DATA_DIR))\n",
    "\n",
    "    new_accuracy, _ = evaluate(X_new, y_new)\n",
    "    print(\"New Data Accuracy = {:.3f}\".format(new_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation to retrieve the top 5 softmax probabilities\n",
    "topk_operation = tf.nn.top_k(tf.nn.softmax(logits), k=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(DATA_DIR))\n",
    "    top5_softmax = sess.run(topk_operation, feed_dict={x: X_new, y: y_new, dropout_rate: 0.0})\n",
    "\n",
    "\n",
    "# Prepare subplot grid.\n",
    "ncols = 5\n",
    "nrows = (len(X_new) + ncols - 1) // ncols\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,7), squeeze=True)\n",
    "\n",
    "# Plot each image on the grid.\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    # i runs from 0 to (nrows*ncols-1)\n",
    "    if i >= len(X_new):\n",
    "        fig.delaxes(axi)\n",
    "        continue\n",
    "\n",
    "    axi.imshow(X_new[i], cmap=\"gray\")\n",
    "    axi.set_title(\"Image {:02d}\".format(i+1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web.\n",
    "for i in range(len(X_new)):\n",
    "    top5 = dict(zip(top5_softmax.indices[i], top5_softmax.values[i]))\n",
    "    print(\"Image #{:02d} with label {:02d} was classified as {:02d}.\".format(i+1, y_new[i], top5_softmax.indices[i][0]))\n",
    "    print(\"Top 5 Probabilities:\", \", \".join(\"{}: {:.2%}\".format(k, v) for k, v in top5.items()))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Visualize the Neural Network's State with Test Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1, plt_num=1):\n",
    "    \"\"\"\n",
    "    Plot a feature map or our CNN to visualize the weights and features the network has identified after training.\n",
    "\n",
    "    :param image_input: the test image being fed into the network to produce the feature maps\n",
    "    :param tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "    :param activation_min: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "    :param activation_max: see ``activation_min``\n",
    "    :param plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input, dropout_rate: 0.0})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")\n",
    "\n",
    "# Print feature maps for convolution layer 1 and 2 for each of the 5 downloaded images.\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(DATA_DIR))\n",
    "\n",
    "    for i, img in enumerate(X_new):\n",
    "\n",
    "        outputFeatureMap([img], conv1, plt_num=i*2+1)\n",
    "        outputFeatureMap([img], conv2, plt_num=i*2+2)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
